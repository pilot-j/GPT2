{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Byte Pair Encoding","metadata":{}},{"cell_type":"code","source":"#Unicode\n#Strings are immutable sequences of Unicode code points. \n\nvar = \"Hello world !\"\n\nfor word in var:\n    # ord ---> returns unicode for character not string tho!\n    print(ord(word))\n\nunicode_list = [(ord(x)) for x in var]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T19:06:35.131275Z","iopub.execute_input":"2024-12-31T19:06:35.131663Z","iopub.status.idle":"2024-12-31T19:06:35.144184Z","shell.execute_reply.started":"2024-12-31T19:06:35.131621Z","shell.execute_reply":"2024-12-31T19:06:35.142744Z"}},"outputs":[{"name":"stdout","text":"72\n101\n108\n108\n111\n32\n119\n111\n114\n108\n100\n32\n33\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"unicode_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:50:10.906293Z","iopub.execute_input":"2024-12-31T11:50:10.906637Z","iopub.status.idle":"2024-12-31T11:50:10.913815Z","shell.execute_reply.started":"2024-12-31T11:50:10.906609Z","shell.execute_reply":"2024-12-31T11:50:10.912814Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"[72, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 32, 33]"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"base_para = str(\"Artificial intelligence, or AI, is concerned with building systems that simulate intelligent behavior. It encompasses a wide range of approaches, including those based on logic, search, and probabilistic reasoning. Machine learning is a subset of AI that learns to make decisions by fitting mathematical models to observed data. This area has seen explosive growth and is now (incorrectly) almost synonymous with the term AI. A deep neural network (or deep network for short) is a type of machine learning model, and the process of fitting these models to data is referred to as deep learning. At the time of writing, deep networks are the most powerful and practical machine learning models and are often encountered in day-to-day life. It is commonplace to translate text to another language using a natural language processing algorithm, to search for images of a given object using a computer vision system, or to converse with a digital assistant via a speech recognition interface. All of these applications are powered by deep learning.\")\nbyte_object = base_para[:500].encode(\"utf-8\")\nprint(byte_object, len(byte_object))\n\nprint(\"_____________\")\nbyte_list = list(byte_object) #list(map(int, tokens))\nprint(byte_list, len(byte_list))\nprint(\"_____________\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T19:06:39.185489Z","iopub.execute_input":"2024-12-31T19:06:39.186002Z","iopub.status.idle":"2024-12-31T19:06:39.195496Z","shell.execute_reply.started":"2024-12-31T19:06:39.185967Z","shell.execute_reply":"2024-12-31T19:06:39.194170Z"}},"outputs":[{"name":"stdout","text":"b'Artificial intelligence, or AI, is concerned with building systems that simulate intelligent behavior. It encompasses a wide range of approaches, including those based on logic, search, and probabilistic reasoning. Machine learning is a subset of AI that learns to make decisions by fitting mathematical models to observed data. This area has seen explosive growth and is now (incorrectly) almost synonymous with the term AI. A deep neural network (or deep network for short) is a type of machine lea' 500\n_____________\n[65, 114, 116, 105, 102, 105, 99, 105, 97, 108, 32, 105, 110, 116, 101, 108, 108, 105, 103, 101, 110, 99, 101, 44, 32, 111, 114, 32, 65, 73, 44, 32, 105, 115, 32, 99, 111, 110, 99, 101, 114, 110, 101, 100, 32, 119, 105, 116, 104, 32, 98, 117, 105, 108, 100, 105, 110, 103, 32, 115, 121, 115, 116, 101, 109, 115, 32, 116, 104, 97, 116, 32, 115, 105, 109, 117, 108, 97, 116, 101, 32, 105, 110, 116, 101, 108, 108, 105, 103, 101, 110, 116, 32, 98, 101, 104, 97, 118, 105, 111, 114, 46, 32, 73, 116, 32, 101, 110, 99, 111, 109, 112, 97, 115, 115, 101, 115, 32, 97, 32, 119, 105, 100, 101, 32, 114, 97, 110, 103, 101, 32, 111, 102, 32, 97, 112, 112, 114, 111, 97, 99, 104, 101, 115, 44, 32, 105, 110, 99, 108, 117, 100, 105, 110, 103, 32, 116, 104, 111, 115, 101, 32, 98, 97, 115, 101, 100, 32, 111, 110, 32, 108, 111, 103, 105, 99, 44, 32, 115, 101, 97, 114, 99, 104, 44, 32, 97, 110, 100, 32, 112, 114, 111, 98, 97, 98, 105, 108, 105, 115, 116, 105, 99, 32, 114, 101, 97, 115, 111, 110, 105, 110, 103, 46, 32, 77, 97, 99, 104, 105, 110, 101, 32, 108, 101, 97, 114, 110, 105, 110, 103, 32, 105, 115, 32, 97, 32, 115, 117, 98, 115, 101, 116, 32, 111, 102, 32, 65, 73, 32, 116, 104, 97, 116, 32, 108, 101, 97, 114, 110, 115, 32, 116, 111, 32, 109, 97, 107, 101, 32, 100, 101, 99, 105, 115, 105, 111, 110, 115, 32, 98, 121, 32, 102, 105, 116, 116, 105, 110, 103, 32, 109, 97, 116, 104, 101, 109, 97, 116, 105, 99, 97, 108, 32, 109, 111, 100, 101, 108, 115, 32, 116, 111, 32, 111, 98, 115, 101, 114, 118, 101, 100, 32, 100, 97, 116, 97, 46, 32, 84, 104, 105, 115, 32, 97, 114, 101, 97, 32, 104, 97, 115, 32, 115, 101, 101, 110, 32, 101, 120, 112, 108, 111, 115, 105, 118, 101, 32, 103, 114, 111, 119, 116, 104, 32, 97, 110, 100, 32, 105, 115, 32, 110, 111, 119, 32, 40, 105, 110, 99, 111, 114, 114, 101, 99, 116, 108, 121, 41, 32, 97, 108, 109, 111, 115, 116, 32, 115, 121, 110, 111, 110, 121, 109, 111, 117, 115, 32, 119, 105, 116, 104, 32, 116, 104, 101, 32, 116, 101, 114, 109, 32, 65, 73, 46, 32, 65, 32, 100, 101, 101, 112, 32, 110, 101, 117, 114, 97, 108, 32, 110, 101, 116, 119, 111, 114, 107, 32, 40, 111, 114, 32, 100, 101, 101, 112, 32, 110, 101, 116, 119, 111, 114, 107, 32, 102, 111, 114, 32, 115, 104, 111, 114, 116, 41, 32, 105, 115, 32, 97, 32, 116, 121, 112, 101, 32, 111, 102, 32, 109, 97, 99, 104, 105, 110, 101, 32, 108, 101, 97] 500\n_____________\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import regex\nfrom collections import defaultdict\nfrom typing import List, Dict, Tuple, Any\n\n\nclass BytePairEncoder:\n    \"\"\"\n    A class implementing Byte Pair Encoding (BPE) compression algorithm.\n    \n    BPE iteratively replaces the most frequent pair of consecutive bytes/tokens\n    with a new token, effectively compressing the data by reducing repeated patterns.\n    \"\"\"\n    \n    def __init__(self, lower_frequency_limit: int = 1, max_iterations: int = 100, vocabulary:Dict[Tuple[Any, Any], int] = {} ):\n        \"\"\"\n        Initialize the BPE encoder.\n        \n        Args:\n            lower_frequency_limit: Minimum frequency threshold for pair merging\n            max_iterations: Maximum number of compression iterations\n            vocabulary: predefined dictionary of current vocabulary in use\n        \"\"\"\n        self.lower_frequency_limit = lower_frequency_limit\n        self.max_iterations = max_iterations\n        self.vocab = vocabulary\n        \n    def _count_pair_frequencies(self, sequence: List[int]) -> Dict[Tuple[Any, Any], int]:\n        \"\"\"\n        Count frequencies of adjacent pairs in the sequence.\n        \n        Args:\n            sequence: List of tokens to analyze\n            \n        Returns:\n            Dictionary mapping token pairs to their frequencies\n        \"\"\"\n        freq_map = defaultdict(int)\n        for i in range(len(sequence) - 1):\n            pair = (sequence[i], sequence[i + 1])\n            freq_map[pair] += 1\n        return freq_map\n    \n    def _find_most_frequent_pairs(self, freq_map: Dict[Tuple[Any, Any], int]) -> List[Tuple[Any, Any]]:\n        \"\"\"\n        Find pairs that occur most frequently, above the lower frequency limit.\n        \n        Args:\n            freq_map: Dictionary of pair frequencies\n            \n        Returns:\n            List of pairs that meet the frequency criteria\n        \"\"\"\n        if not freq_map:\n            return []\n        most_freq_pairs = [x for x in freq_map.keys() if freq_map[x] == max(max(freq_map.values()), self.lower_frequency_limit)]\n        return most_freq_pairs\n\n    \n\n    \n    def _update_vocabulary(self, pairs):\n        \"\"\"\n        Assign new token values to frequent pairs in the vocabulary.\n        \n        Args:\n            pairs: List of pairs to add to vocabulary\n        \"\"\"\n        if not self.vocab:\n            next_token = 0\n        else:\n            next_token = max(self.vocab.values()) + 1\n            \n        for pair in pairs:\n            self.vocab[pair] = next_token\n            next_token += 1\n    \n    def _compress_sequence(self, sequence: List[Any], vocab) -> List[Any]:\n        \"\"\"\n        Compress the sequence by replacing frequent pairs with their token values.\n        \n        Args:\n            sequence: List of tokens to compress\n            vocabulary\n            \n        Returns:\n            Compressed sequence\n        \"\"\"\n        compressed = []\n        i, flag = 0, 0\n        while i < len(sequence) - 1:\n            pair = (sequence[i], sequence[i + 1])\n            if pair in vocab:\n                flag = 1\n                compressed.append(vocab[pair])\n                i += 2\n            else:\n                compressed.append(sequence[i])\n                i += 1\n                \n        # Handle last token if we're at the end\n        if i == len(sequence) - 1:\n            compressed.append(sequence[-1])\n\n        return compressed\n\n    def regex_chunking(self, text: str) -> List[str]:\n        pattern = regex.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n        text_chunks = regex.findall(pattern , text)\n        return text_chunks\n    \n    def train_bpe(self, train_text: str):\n        \"\"\"\n        Encode the input sequence using BPE algorithm.\n        \n        Args:\n            sequence: Input sequence to compress\n            \n        Returns:\n            None\n        \"\"\"\n        text_chunks = self.regex_chunking(train_text)\n        \n        current_sequence_list = [list(chunk.encode('utf-8')) for chunk in text_chunks]\n        \n        \n        \n        for i in range(self.max_iterations):\n            freq_map = defaultdict(int)\n            \n            # Count frequencies of adjacent pairs\n            for seq in current_sequence_list:\n                for k, v in self._count_pair_frequencies(seq).items():\n                    freq_map[k] += v\n\n            #Find frequent pairs. Note multiple pairs can have same frequency.\n            frequent_pairs = self._find_most_frequent_pairs(freq_map)\n            \n           \n            # If no pairs meet criteria, stop iteration\n            if not frequent_pairs:\n                print(\"No more frequent pairs found. Training completed!\")\n                break\n                \n            # Update vocabulary with new tokens\n            self._update_vocabulary(frequent_pairs)\n\n          \n            # Compress sequence using updated vocabulary\n            new_sequence_list = [self._compress_sequence(seq, self.vocab) for seq in current_sequence_list]   \n            current_sequence_list = new_sequence_list\n\n        print(f\"length of training corpus {len(train_text)}, #new_tokens added:{len(self.vocab)-256}. Training completed\")\n        \n\n    def encode(self, text):\n        pass\n        \n            \n    def get_vocabulary(self) -> Dict[Tuple[Any, Any], int]:\n        \"\"\"\n        Get the current BPE vocabulary.\n        \n        Returns:\n            Dictionary mapping token pairs to their assigned values\n        \"\"\"\n        return self.vocab.copy()\n\n    \n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T19:16:40.856367Z","iopub.execute_input":"2024-12-31T19:16:40.856850Z","iopub.status.idle":"2024-12-31T19:16:40.874731Z","shell.execute_reply.started":"2024-12-31T19:16:40.856819Z","shell.execute_reply":"2024-12-31T19:16:40.873264Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T17:55:31.987334Z","iopub.execute_input":"2024-12-31T17:55:31.987681Z","iopub.status.idle":"2024-12-31T17:55:31.994320Z","shell.execute_reply.started":"2024-12-31T17:55:31.987654Z","shell.execute_reply":"2024-12-31T17:55:31.993039Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'a'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"lower_lim = 3\nmax_iter = 5\ninitial_vocab = {chr(i):i for i in range(256)}\nBPE = BytePairEncoder(lower_lim, max_iter, initial_vocab)\ncom_seq = BPE.train_bpe(base_para)\nvocabulary = BPE.get_vocabulary()\nprint(vocabulary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T19:13:00.208919Z","iopub.execute_input":"2024-12-31T19:13:00.209397Z","iopub.status.idle":"2024-12-31T19:13:00.229654Z","shell.execute_reply.started":"2024-12-31T19:13:00.209361Z","shell.execute_reply":"2024-12-31T19:13:00.228390Z"}},"outputs":[{"name":"stdout","text":"length of training corpus 1043, #new_tokens added:5. Training completed\n{'\\x00': 0, '\\x01': 1, '\\x02': 2, '\\x03': 3, '\\x04': 4, '\\x05': 5, '\\x06': 6, '\\x07': 7, '\\x08': 8, '\\t': 9, '\\n': 10, '\\x0b': 11, '\\x0c': 12, '\\r': 13, '\\x0e': 14, '\\x0f': 15, '\\x10': 16, '\\x11': 17, '\\x12': 18, '\\x13': 19, '\\x14': 20, '\\x15': 21, '\\x16': 22, '\\x17': 23, '\\x18': 24, '\\x19': 25, '\\x1a': 26, '\\x1b': 27, '\\x1c': 28, '\\x1d': 29, '\\x1e': 30, '\\x1f': 31, ' ': 32, '!': 33, '\"': 34, '#': 35, '$': 36, '%': 37, '&': 38, \"'\": 39, '(': 40, ')': 41, '*': 42, '+': 43, ',': 44, '-': 45, '.': 46, '/': 47, '0': 48, '1': 49, '2': 50, '3': 51, '4': 52, '5': 53, '6': 54, '7': 55, '8': 56, '9': 57, ':': 58, ';': 59, '<': 60, '=': 61, '>': 62, '?': 63, '@': 64, 'A': 65, 'B': 66, 'C': 67, 'D': 68, 'E': 69, 'F': 70, 'G': 71, 'H': 72, 'I': 73, 'J': 74, 'K': 75, 'L': 76, 'M': 77, 'N': 78, 'O': 79, 'P': 80, 'Q': 81, 'R': 82, 'S': 83, 'T': 84, 'U': 85, 'V': 86, 'W': 87, 'X': 88, 'Y': 89, 'Z': 90, '[': 91, '\\\\': 92, ']': 93, '^': 94, '_': 95, '`': 96, 'a': 97, 'b': 98, 'c': 99, 'd': 100, 'e': 101, 'f': 102, 'g': 103, 'h': 104, 'i': 105, 'j': 106, 'k': 107, 'l': 108, 'm': 109, 'n': 110, 'o': 111, 'p': 112, 'q': 113, 'r': 114, 's': 115, 't': 116, 'u': 117, 'v': 118, 'w': 119, 'x': 120, 'y': 121, 'z': 122, '{': 123, '|': 124, '}': 125, '~': 126, '\\x7f': 127, '\\x80': 128, '\\x81': 129, '\\x82': 130, '\\x83': 131, '\\x84': 132, '\\x85': 133, '\\x86': 134, '\\x87': 135, '\\x88': 136, '\\x89': 137, '\\x8a': 138, '\\x8b': 139, '\\x8c': 140, '\\x8d': 141, '\\x8e': 142, '\\x8f': 143, '\\x90': 144, '\\x91': 145, '\\x92': 146, '\\x93': 147, '\\x94': 148, '\\x95': 149, '\\x96': 150, '\\x97': 151, '\\x98': 152, '\\x99': 153, '\\x9a': 154, '\\x9b': 155, '\\x9c': 156, '\\x9d': 157, '\\x9e': 158, '\\x9f': 159, '\\xa0': 160, '¡': 161, '¢': 162, '£': 163, '¤': 164, '¥': 165, '¦': 166, '§': 167, '¨': 168, '©': 169, 'ª': 170, '«': 171, '¬': 172, '\\xad': 173, '®': 174, '¯': 175, '°': 176, '±': 177, '²': 178, '³': 179, '´': 180, 'µ': 181, '¶': 182, '·': 183, '¸': 184, '¹': 185, 'º': 186, '»': 187, '¼': 188, '½': 189, '¾': 190, '¿': 191, 'À': 192, 'Á': 193, 'Â': 194, 'Ã': 195, 'Ä': 196, 'Å': 197, 'Æ': 198, 'Ç': 199, 'È': 200, 'É': 201, 'Ê': 202, 'Ë': 203, 'Ì': 204, 'Í': 205, 'Î': 206, 'Ï': 207, 'Ð': 208, 'Ñ': 209, 'Ò': 210, 'Ó': 211, 'Ô': 212, 'Õ': 213, 'Ö': 214, '×': 215, 'Ø': 216, 'Ù': 217, 'Ú': 218, 'Û': 219, 'Ü': 220, 'Ý': 221, 'Þ': 222, 'ß': 223, 'à': 224, 'á': 225, 'â': 226, 'ã': 227, 'ä': 228, 'å': 229, 'æ': 230, 'ç': 231, 'è': 232, 'é': 233, 'ê': 234, 'ë': 235, 'ì': 236, 'í': 237, 'î': 238, 'ï': 239, 'ð': 240, 'ñ': 241, 'ò': 242, 'ó': 243, 'ô': 244, 'õ': 245, 'ö': 246, '÷': 247, 'ø': 248, 'ù': 249, 'ú': 250, 'û': 251, 'ü': 252, 'ý': 253, 'þ': 254, 'ÿ': 255, (32, 97): 256, (105, 110): 257, (32, 116): 258, (257, 103): 259, (32, 111): 260}\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def detect_circular_mappings(vocab):\n    for pair, token in vocab.items():\n        if (token,) in vocab:\n            \n            print(f\"Circular mapping detected: {pair} -> {token}\")\n\n    print(\"no\")\n\ndetect_circular_mappings(vocabulary)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T19:11:14.391338Z","iopub.execute_input":"2024-12-31T19:11:14.391718Z","iopub.status.idle":"2024-12-31T19:11:14.399508Z","shell.execute_reply.started":"2024-12-31T19:11:14.391688Z","shell.execute_reply":"2024-12-31T19:11:14.397589Z"}},"outputs":[{"name":"stdout","text":"no\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"text = \"My name is Hello World!\"\nencoded = BPE.encode(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T19:16:47.837724Z","iopub.execute_input":"2024-12-31T19:16:47.838181Z","iopub.status.idle":"2024-12-31T19:16:47.844538Z","shell.execute_reply.started":"2024-12-31T19:16:47.838144Z","shell.execute_reply":"2024-12-31T19:16:47.843012Z"}},"outputs":[{"name":"stdout","text":"initial byte list: [77, 121, 32, 110, 97, 109, 101, 32, 105, 115, 32, 72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100, 33]\ncompressed byte list: [77, 121, 32, 110, 97, 109, 101, 32, 105, 115, 32, 72, 101, 108, 108, 111, 32, 87, 111, 114, 108, 100, 33]\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(com_seq)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-31T11:53:22.141887Z","iopub.execute_input":"2024-12-31T11:53:22.142271Z","iopub.status.idle":"2024-12-31T11:53:22.146974Z","shell.execute_reply.started":"2024-12-31T11:53:22.142213Z","shell.execute_reply":"2024-12-31T11:53:22.145812Z"}},"outputs":[{"name":"stdout","text":"[65, 114, 116, 105, 102, 269, 105, 270, 32, 257, 271, 108, 282, 103, 272, 99, 101, 273, 259, 283, 273, 265, 99, 266, 99, 101, 114, 275, 267, 284, 260, 32, 98, 117, 105, 108, 100, 268, 261, 121, 115, 271, 109, 256, 260, 97, 262, 285, 109, 117, 108, 97, 116, 258, 257, 271, 108, 282, 103, 272, 262, 98, 101, 104, 97, 118, 105, 259, 276, 73, 262, 272, 99, 111, 109, 112, 97, 115, 263, 256, 97, 32, 284, 100, 258, 114, 97, 110, 103, 258, 286, 277, 112, 112, 287, 288, 101, 115, 273, 257, 99, 108, 117, 100, 268, 289, 290, 258, 98, 97, 263, 267, 266, 32, 108, 111, 103, 269, 44, 261, 291, 278, 273, 97, 110, 267, 112, 287, 98, 97, 98, 105, 282, 115, 116, 269, 32, 114, 264, 115, 266, 268, 276, 77, 288, 257, 258, 292, 114, 110, 268, 32, 293, 261, 117, 98, 263, 262, 286, 283, 289, 97, 262, 292, 114, 110, 256, 116, 111, 294, 107, 258, 281, 99, 105, 285, 266, 256, 98, 121, 32, 102, 105, 116, 116, 268, 294, 260, 101, 280, 116, 269, 270, 279, 111, 281, 108, 256, 116, 111, 32, 111, 98, 263, 114, 118, 101, 267, 100, 97, 116, 97, 276, 84, 104, 293, 114, 264, 32, 104, 97, 256, 263, 272, 32, 101, 120, 112, 108, 290, 105, 118, 258, 103, 287, 119, 260, 277, 110, 267, 265, 110, 111, 119, 32, 40, 257, 99, 259, 114, 101, 99, 116, 108, 121, 41, 277, 108, 109, 290, 262, 115, 121, 110, 266, 121, 109, 111, 117, 256, 284, 260, 289, 258, 271, 114, 109, 283, 276, 65, 32, 281, 101, 112, 295, 117, 114, 270, 295, 116, 119, 259, 107, 32, 40, 259, 32, 281, 101, 112, 295, 116, 119, 259, 107, 32, 102, 259, 261, 104, 259, 116, 41, 32, 293, 32, 116, 121, 112, 258, 286, 294, 278, 257, 258, 292]\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"from collections import defaultdict\n\ndef foo(byte_list, lower_lim, vocab, max_iterations):\n    iteration = 0\n    while iteration < max_iterations:\n        if iteration > 0: byte_list = new_byte_list\n        freq_map = defaultdict(int) #default value for int is 0\n        \n        # Count frequencies\n        for i in range(len(byte_list)-1):\n            pair = (byte_list[i], byte_list[i+1])\n            freq_map[pair] += 1\n        \n        #get most frequently occuring pairs\n        most_freq_pairs = [x for x in freq_map.keys() if freq_map[x] == max(max(freq_map.values()), lower_lim)]\n\n    \n        #create a dictionary of the identified pairs and allot them a token value\n        last_token = vocab[list(vocab.keys())[-1]]\n\n        for i, pair in enumerate(most_freq_pairs):\n            print(i+last_token)\n            vocab[pair]= i + last_token +1\n        \n        \n        #compress the current byte list\n        new_byte_list = []\n        \n        i = 0\n        while i < len(byte_list)-1:\n            pair = (byte_list[i], byte_list[i+1])\n            if pair in most_freq_pairs:\n                new_byte_list.append(vocab[pair])\n                i += 2  # Skip both tokens in the pair\n            else:\n                new_byte_list.append(byte_list[i])\n                i += 1\n                \n            if i == len(byte_list)-1:  # Handle last token\n                new_byte_list.append(byte_list[-1])\n        iteration+=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T17:04:56.684868Z","iopub.execute_input":"2024-12-29T17:04:56.685186Z","iopub.status.idle":"2024-12-29T17:04:56.693198Z","shell.execute_reply.started":"2024-12-29T17:04:56.685162Z","shell.execute_reply":"2024-12-29T17:04:56.692068Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"lower_lim =4\nmax_iterations = 5\nloop = 0\nfoo(byte_list, lower_lim, initial_vocab, max_iterations )\n\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T17:04:59.505057Z","iopub.execute_input":"2024-12-29T17:04:59.505373Z","iopub.status.idle":"2024-12-29T17:04:59.523544Z","shell.execute_reply.started":"2024-12-29T17:04:59.505347Z","shell.execute_reply":"2024-12-29T17:04:59.522425Z"}},"outputs":[{"name":"stdout","text":"[(115, 32)]\nÿ 255\n255\n[(105, 110)]\n(115, 32) 256\n256\n[(101, 32)]\n(105, 110) 257\n257\n[(111, 114), (116, 104)]\n(101, 32) 258\n258\n259\n[(32, 115), (116, 32), (115, 101), (101, 97)]\n(116, 104) 260\n260\n261\n262\n263\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"print(initial_vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T17:05:16.345916Z","iopub.execute_input":"2024-12-29T17:05:16.346238Z","iopub.status.idle":"2024-12-29T17:05:16.351831Z","shell.execute_reply.started":"2024-12-29T17:05:16.346213Z","shell.execute_reply":"2024-12-29T17:05:16.350324Z"}},"outputs":[{"name":"stdout","text":"{'\\x00': 0, '\\x01': 1, '\\x02': 2, '\\x03': 3, '\\x04': 4, '\\x05': 5, '\\x06': 6, '\\x07': 7, '\\x08': 8, '\\t': 9, '\\n': 10, '\\x0b': 11, '\\x0c': 12, '\\r': 13, '\\x0e': 14, '\\x0f': 15, '\\x10': 16, '\\x11': 17, '\\x12': 18, '\\x13': 19, '\\x14': 20, '\\x15': 21, '\\x16': 22, '\\x17': 23, '\\x18': 24, '\\x19': 25, '\\x1a': 26, '\\x1b': 27, '\\x1c': 28, '\\x1d': 29, '\\x1e': 30, '\\x1f': 31, ' ': 32, '!': 33, '\"': 34, '#': 35, '$': 36, '%': 37, '&': 38, \"'\": 39, '(': 40, ')': 41, '*': 42, '+': 43, ',': 44, '-': 45, '.': 46, '/': 47, '0': 48, '1': 49, '2': 50, '3': 51, '4': 52, '5': 53, '6': 54, '7': 55, '8': 56, '9': 57, ':': 58, ';': 59, '<': 60, '=': 61, '>': 62, '?': 63, '@': 64, 'A': 65, 'B': 66, 'C': 67, 'D': 68, 'E': 69, 'F': 70, 'G': 71, 'H': 72, 'I': 73, 'J': 74, 'K': 75, 'L': 76, 'M': 77, 'N': 78, 'O': 79, 'P': 80, 'Q': 81, 'R': 82, 'S': 83, 'T': 84, 'U': 85, 'V': 86, 'W': 87, 'X': 88, 'Y': 89, 'Z': 90, '[': 91, '\\\\': 92, ']': 93, '^': 94, '_': 95, '`': 96, 'a': 97, 'b': 98, 'c': 99, 'd': 100, 'e': 101, 'f': 102, 'g': 103, 'h': 104, 'i': 105, 'j': 106, 'k': 107, 'l': 108, 'm': 109, 'n': 110, 'o': 111, 'p': 112, 'q': 113, 'r': 114, 's': 115, 't': 116, 'u': 117, 'v': 118, 'w': 119, 'x': 120, 'y': 121, 'z': 122, '{': 123, '|': 124, '}': 125, '~': 126, '\\x7f': 127, '\\x80': 128, '\\x81': 129, '\\x82': 130, '\\x83': 131, '\\x84': 132, '\\x85': 133, '\\x86': 134, '\\x87': 135, '\\x88': 136, '\\x89': 137, '\\x8a': 138, '\\x8b': 139, '\\x8c': 140, '\\x8d': 141, '\\x8e': 142, '\\x8f': 143, '\\x90': 144, '\\x91': 145, '\\x92': 146, '\\x93': 147, '\\x94': 148, '\\x95': 149, '\\x96': 150, '\\x97': 151, '\\x98': 152, '\\x99': 153, '\\x9a': 154, '\\x9b': 155, '\\x9c': 156, '\\x9d': 157, '\\x9e': 158, '\\x9f': 159, '\\xa0': 160, '¡': 161, '¢': 162, '£': 163, '¤': 164, '¥': 165, '¦': 166, '§': 167, '¨': 168, '©': 169, 'ª': 170, '«': 171, '¬': 172, '\\xad': 173, '®': 174, '¯': 175, '°': 176, '±': 177, '²': 178, '³': 179, '´': 180, 'µ': 181, '¶': 182, '·': 183, '¸': 184, '¹': 185, 'º': 186, '»': 187, '¼': 188, '½': 189, '¾': 190, '¿': 191, 'À': 192, 'Á': 193, 'Â': 194, 'Ã': 195, 'Ä': 196, 'Å': 197, 'Æ': 198, 'Ç': 199, 'È': 200, 'É': 201, 'Ê': 202, 'Ë': 203, 'Ì': 204, 'Í': 205, 'Î': 206, 'Ï': 207, 'Ð': 208, 'Ñ': 209, 'Ò': 210, 'Ó': 211, 'Ô': 212, 'Õ': 213, 'Ö': 214, '×': 215, 'Ø': 216, 'Ù': 217, 'Ú': 218, 'Û': 219, 'Ü': 220, 'Ý': 221, 'Þ': 222, 'ß': 223, 'à': 224, 'á': 225, 'â': 226, 'ã': 227, 'ä': 228, 'å': 229, 'æ': 230, 'ç': 231, 'è': 232, 'é': 233, 'ê': 234, 'ë': 235, 'ì': 236, 'í': 237, 'î': 238, 'ï': 239, 'ð': 240, 'ñ': 241, 'ò': 242, 'ó': 243, 'ô': 244, 'õ': 245, 'ö': 246, '÷': 247, 'ø': 248, 'ù': 249, 'ú': 250, 'û': 251, 'ü': 252, 'ý': 253, 'þ': 254, 'ÿ': 255, (115, 32): 256, (105, 110): 257, (101, 32): 258, (111, 114): 259, (116, 104): 260, (32, 115): 261, (116, 32): 262, (115, 101): 263, (101, 97): 264}\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"print(500 - len(compressed_list))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T16:49:42.554733Z","iopub.execute_input":"2024-12-29T16:49:42.555079Z","iopub.status.idle":"2024-12-29T16:49:42.560593Z","shell.execute_reply.started":"2024-12-29T16:49:42.555051Z","shell.execute_reply":"2024-12-29T16:49:42.559547Z"}},"outputs":[{"name":"stdout","text":"147\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"print(new_list_1[:10])\nprint(new_list[:10])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-29T16:48:11.321241Z","iopub.execute_input":"2024-12-29T16:48:11.321622Z","iopub.status.idle":"2024-12-29T16:48:11.331819Z","shell.execute_reply.started":"2024-12-29T16:48:11.321587Z","shell.execute_reply":"2024-12-29T16:48:11.330460Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-d679e18cdbbc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_list_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'new_list_1' is not defined"],"ename":"NameError","evalue":"name 'new_list_1' is not defined","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}